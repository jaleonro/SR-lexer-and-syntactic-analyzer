{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizador sintáctico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "\n",
    "class Token:\n",
    "  def __init__(self, lexema, tipo, fila, col):\n",
    "    self.lexema = lexema\n",
    "    self.tipo = tipo\n",
    "    self.fila = fila\n",
    "    self.col = col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regla:    \n",
    "    def __init__(self,no_terminal,derivacion):\n",
    "        self.no_terminal = no_terminal\n",
    "        self.derivacion = derivacion\n",
    "        \n",
    "    def _primeros(self,alfa,gramatica):\n",
    "        prim = set()\n",
    "        if alfa == ['Ñ']:\n",
    "            prim.add('Ñ')\n",
    "            return prim\n",
    "        else:\n",
    "            a1 = alfa[0]\n",
    "            if a1.isupper():\n",
    "                prim.add(a1)\n",
    "                return prim\n",
    "            else:\n",
    "                reglas_de_a1 = gramatica[a1]\n",
    "                primeros_a1 = set()\n",
    "                for regla in reglas_de_a1:\n",
    "                    primeros_a1 = primeros_a1 | self._primeros(regla.derivacion,gramatica)                \n",
    "                copia_primeros_a1 = primeros_a1.copy()\n",
    "                copia_primeros_a1.discard('Ñ')                \n",
    "                prim = prim | copia_primeros_a1                \n",
    "                if 'Ñ' in primeros_a1:                                   \n",
    "                    if len(alfa) == 1:\n",
    "                        prim.add('Ñ')                        \n",
    "                        return prim\n",
    "                    else:                         \n",
    "                        alfa2 = alfa.copy()\n",
    "                        alfa2.pop(0)                        \n",
    "                        prim = prim | self._primeros(alfa2,gramatica)                         \n",
    "        return prim\n",
    "\n",
    "    def _siguientes(self,A,gramatica):\n",
    "        sig = set()\n",
    "        if A == list(gramatica.keys())[0]:\n",
    "            sig.add('ÑÑ')\n",
    "        for no_terminal in gramatica.keys():\n",
    "            reglas = gramatica[no_terminal]\n",
    "            for regla in reglas: \n",
    "                der = regla.derivacion\n",
    "                if A in der:                    \n",
    "                    while True:\n",
    "                        len1 = len(sig)\n",
    "                        if der[-1] == A:                            \n",
    "                            sig = sig | self._siguientes(no_terminal,gramatica)                            \n",
    "                        else:\n",
    "                            lista = []\n",
    "                            lista.append(der[der.index(A)+1])\n",
    "                            primeros_de_beta = self._primeros(lista,gramatica)                            \n",
    "                            primeros_de_beta2 = primeros_de_beta.copy()\n",
    "                            primeros_de_beta2.discard('Ñ')\n",
    "                            sig = sig | primeros_de_beta2                            \n",
    "                            if 'Ñ' in primeros_de_beta:\n",
    "                                sig = sig | self._siguientes(no_terminal,gramatica)                                \n",
    "                        len2 = len(sig)\n",
    "                        if len1 == len2:\n",
    "                            return sig\n",
    "        return sig\n",
    "\n",
    "    def conjunto_prediccion(self,gramatica):\n",
    "        conj_pred = set()\n",
    "        alfa = self.derivacion\n",
    "        no_terminal = self.no_terminal\n",
    "        prim_alfa = self._primeros(alfa,gramatica)\n",
    "        if 'Ñ' in prim_alfa:            \n",
    "            prim_alfa.discard('Ñ')\n",
    "            conj_pred = prim_alfa | self._siguientes(no_terminal,gramatica)\n",
    "        else:\n",
    "            conj_pred = self._primeros(alfa,gramatica)\n",
    "        return conj_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'program': [<__main__.Regla at 0x1be49238668>],\n",
       " 'component_ls': [<__main__.Regla at 0x1be492386a0>,\n",
       "  <__main__.Regla at 0x1be492386d8>],\n",
       " 'component': [<__main__.Regla at 0x1be49238710>,\n",
       "  <__main__.Regla at 0x1be49238748>,\n",
       "  <__main__.Regla at 0x1be49238780>,\n",
       "  <__main__.Regla at 0x1be492387b8>],\n",
       " 'spec_component': [<__main__.Regla at 0x1be492387f0>],\n",
       " 'combined_component': [<__main__.Regla at 0x1be49238828>],\n",
       " 'combined_specpart': [<__main__.Regla at 0x1be49238860>],\n",
       " 'comp_label': [<__main__.Regla at 0x1be49238898>],\n",
       " 'comp_kwd': [<__main__.Regla at 0x1be492388d0>,\n",
       "  <__main__.Regla at 0x1be49238908>],\n",
       " 'spec_body': [<__main__.Regla at 0x1be49238940>,\n",
       "  <__main__.Regla at 0x1be49238978>,\n",
       "  <__main__.Regla at 0x1be492389b0>],\n",
       " 'maybe_params': [<__main__.Regla at 0x1be492389e8>,\n",
       "  <__main__.Regla at 0x1be49238a20>],\n",
       " 'separate_body': [<__main__.Regla at 0x1be49238a58>],\n",
       " 'spec_stmt_ls': [<__main__.Regla at 0x1be49238a90>],\n",
       " 'spec_stmt_ls2': [<__main__.Regla at 0x1be49238ac8>,\n",
       "  <__main__.Regla at 0x1be49238b00>],\n",
       " 'spec_stmt': [<__main__.Regla at 0x1be49238b38>,\n",
       "  <__main__.Regla at 0x1be49238b70>,\n",
       "  <__main__.Regla at 0x1be49238ba8>],\n",
       " 'body_stmt_ls': [<__main__.Regla at 0x1be49238be0>],\n",
       " 'body_stmt_ls2': [<__main__.Regla at 0x1be49238c18>,\n",
       "  <__main__.Regla at 0x1be49238c50>],\n",
       " 'body_stmt': [<__main__.Regla at 0x1be49238c88>,\n",
       "  <__main__.Regla at 0x1be49238cc0>,\n",
       "  <__main__.Regla at 0x1be49238cf8>,\n",
       "  <__main__.Regla at 0x1be49238d30>],\n",
       " 'body_only': [<__main__.Regla at 0x1be49238d68>,\n",
       "  <__main__.Regla at 0x1be49238da0>,\n",
       "  <__main__.Regla at 0x1be49238dd8>,\n",
       "  <__main__.Regla at 0x1be49238e10>,\n",
       "  <__main__.Regla at 0x1be49238e48>,\n",
       "  <__main__.Regla at 0x1be49238e80>],\n",
       " 'common_stmt': [<__main__.Regla at 0x1be49238eb8>,\n",
       "  <__main__.Regla at 0x1be49238ef0>,\n",
       "  <__main__.Regla at 0x1be49238f28>],\n",
       " 'import_clause': [<__main__.Regla at 0x1be49238f60>],\n",
       " 'extend_clause': [<__main__.Regla at 0x1be49238f98>],\n",
       " 'import_list': [<__main__.Regla at 0x1be49238fd0>],\n",
       " 'import_list2': [<__main__.Regla at 0x1be49241048>,\n",
       "  <__main__.Regla at 0x1be49241080>],\n",
       " 'import_name': [<__main__.Regla at 0x1be492410b8>],\n",
       " 'end_id': [<__main__.Regla at 0x1be492410f0>],\n",
       " 'id_opt': [<__main__.Regla at 0x1be49241128>,\n",
       "  <__main__.Regla at 0x1be49241160>],\n",
       " 'decl': [<__main__.Regla at 0x1be49241198>,\n",
       "  <__main__.Regla at 0x1be492411d0>,\n",
       "  <__main__.Regla at 0x1be49241208>,\n",
       "  <__main__.Regla at 0x1be49241240>,\n",
       "  <__main__.Regla at 0x1be49241278>],\n",
       " 'type_decl': [<__main__.Regla at 0x1be492412b0>],\n",
       " 'type_restriction': [<__main__.Regla at 0x1be492412e8>,\n",
       "  <__main__.Regla at 0x1be49241320>],\n",
       " 'obj_decl': [<__main__.Regla at 0x1be49241358>],\n",
       " 'var_or_const': [<__main__.Regla at 0x1be49241390>,\n",
       "  <__main__.Regla at 0x1be492413c8>],\n",
       " 'var_def_lp': [<__main__.Regla at 0x1be49241400>],\n",
       " 'var_def_lp2': [<__main__.Regla at 0x1be49241438>,\n",
       "  <__main__.Regla at 0x1be49241470>],\n",
       " 'var_def': [<__main__.Regla at 0x1be492414a8>],\n",
       " 'var_att': [<__main__.Regla at 0x1be492414e0>,\n",
       "  <__main__.Regla at 0x1be49241518>,\n",
       "  <__main__.Regla at 0x1be49241550>],\n",
       " 'var_att2': [<__main__.Regla at 0x1be49241588>,\n",
       "  <__main__.Regla at 0x1be492415c0>],\n",
       " 'id_subs_lp': [<__main__.Regla at 0x1be492415f8>],\n",
       " 'id_subs_lp2': [<__main__.Regla at 0x1be49241630>,\n",
       "  <__main__.Regla at 0x1be49241668>],\n",
       " 'id_subs': [<__main__.Regla at 0x1be492416a0>],\n",
       " 'id_subs2': [<__main__.Regla at 0x1be492416d8>,\n",
       "  <__main__.Regla at 0x1be49241710>],\n",
       " 'subscripts': [<__main__.Regla at 0x1be49241748>],\n",
       " 'subscripts2': [<__main__.Regla at 0x1be49241780>,\n",
       "  <__main__.Regla at 0x1be492417b8>],\n",
       " 'bracketed_list': [<__main__.Regla at 0x1be492417f0>],\n",
       " 'bound_lp': [<__main__.Regla at 0x1be49241828>],\n",
       " 'bound_lp2': [<__main__.Regla at 0x1be49241860>,\n",
       "  <__main__.Regla at 0x1be49241898>],\n",
       " 'bounds': [<__main__.Regla at 0x1be492418d0>],\n",
       " 'bounds2': [<__main__.Regla at 0x1be49241908>,\n",
       "  <__main__.Regla at 0x1be49241940>],\n",
       " 'bound': [<__main__.Regla at 0x1be49241978>,\n",
       "  <__main__.Regla at 0x1be492419b0>],\n",
       " 'expr': [<__main__.Regla at 0x1be492419e8>,\n",
       "  <__main__.Regla at 0x1be49241a20>],\n",
       " 'num': [<__main__.Regla at 0x1be49241a58>, <__main__.Regla at 0x1be49241a90>],\n",
       " 'expr2': [<__main__.Regla at 0x1be49241ac8>,\n",
       "  <__main__.Regla at 0x1be49241b00>],\n",
       " 'operator': [<__main__.Regla at 0x1be49241b38>,\n",
       "  <__main__.Regla at 0x1be49241b70>],\n",
       " 'optype_decl': [<__main__.Regla at 0x1be49241ba8>],\n",
       " 'op_decl': [<__main__.Regla at 0x1be49241be0>],\n",
       " 'error': [<__main__.Regla at 0x1be49241c18>],\n",
       " 'type': [<__main__.Regla at 0x1be49241c50>],\n",
       " 'comp_params': [<__main__.Regla at 0x1be49241c88>],\n",
       " 'parameters': [<__main__.Regla at 0x1be49241cc0>],\n",
       " 'stmt': [<__main__.Regla at 0x1be49241cf8>],\n",
       " 'proc': [<__main__.Regla at 0x1be49241d30>],\n",
       " 'process': [<__main__.Regla at 0x1be49241d68>],\n",
       " 'procedure': [<__main__.Regla at 0x1be49241da0>],\n",
       " 'initial_block': [<__main__.Regla at 0x1be49241dd8>],\n",
       " 'final_block': [<__main__.Regla at 0x1be49241e10>]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gramatica = {'program': [Regla('program',['component','component_ls'])],\n",
    "             'component_ls': [Regla('component_ls',['component','component_ls']),Regla('component_ls',['Ñ'])],\n",
    "             'component': [Regla('component',['spec_component']),Regla('component',['combined_component']),Regla('component',['separate_body']),Regla('component',['error'])],\n",
    "             'spec_component': [Regla('spec_component',['comp_label','spec_stmt_ls','spec_body'])],\n",
    "             'combined_component': [Regla('combined_component',['combined_specpart','body_stmt_ls','end_id'])],\n",
    "             'combined_specpart': [Regla('combined_specpart',['comp_label','parameters'])],\n",
    "             'comp_label': [Regla('comp_label',['comp_kwd','TK_ID'])],             \n",
    "             'comp_kwd': [Regla('comp_kwd',['TK_GLOBAL']),Regla('comp_kwd',['TK_RESOURCE'])],\n",
    "             'spec_body': [Regla('spec_body',['end_id']),Regla('spec_body',['TK_BODY','TK_ID','maybe_params','TK_SEPARATE']),Regla('spec_body',['TK_BODY','TK_ID','maybe_params','body_stmt_ls','end_id'])],\n",
    "             'maybe_params': [Regla('maybe_params',['Ñ']),Regla('maybe_params',['parameters'])],\n",
    "             'separate_body': [Regla('separate_body',['TK_BODY','TK_ID','body_stmt_ls','end_id'])],\n",
    "             'spec_stmt_ls': [Regla('spec_stmt_ls',['spec_stmt','spec_stmt_ls2'])], \n",
    "             'spec_stmt_ls2': [Regla('spec_stmt_ls2',['Ñ']),Regla('spec_stmt_ls2',['TK_SEPARATOR','spec_stmt','spec_stmt_ls2'])],\n",
    "             'spec_stmt': [Regla('spec_stmt',['common_stmt']),Regla('spec_stmt',['extend_clause']),Regla('spec_stmt',['body_only'])],\n",
    "             'body_stmt_ls': [Regla('body_stmt_ls',['body_stmt','body_stmt_ls2'])],\n",
    "             'body_stmt_ls2': [Regla('body_stmt_ls2',['TK_SEPARATOR','body_stmt','body_stmt_ls2']),Regla('spec_stmt_ls2',['Ñ'])],\n",
    "             'body_stmt': [Regla('body_stmt',['common_stmt']),Regla('body_stmt',['expr']),Regla('body_stmt',['body_only']),Regla('body_stmt',['extend_clause'])],\n",
    "             'body_only': [Regla('body_only',['stmt']),Regla('body_only',['proc']),Regla('body_only',['process']),Regla('body_only',['procedure']),Regla('body_only',['initial_block']),Regla('body_only',['final_block'])],\n",
    "             'common_stmt': [Regla('common_stmt',['Ñ']),Regla('common_stmt',['decl']),Regla('common_stmt',['import_clause'])],\n",
    "             'import_clause': [Regla('import_clause',['TK_IMPORT','import_list'])],\n",
    "             'extend_clause': [Regla('extend_clause',['TK_EXTEND','import_list'])],\n",
    "             'import_list': [Regla('import_list',['import_name','import_list2'])],\n",
    "             'import_list2': [Regla('import_list2',['TK_COMMA','import_name','import_list2']),Regla('import_list2',['Ñ'])],\n",
    "             'import_name': [Regla('import_name',['TK_ID'])],\n",
    "             'end_id': [Regla('end_id',['TK_END','id_opt'])],\n",
    "             'id_opt': [Regla('id_opt',['Ñ']),Regla('id_opt',['TK_ID'])],\n",
    "             \n",
    "             'decl': [Regla('decl',['error','TK_SEPARATOR']),Regla('decl',['type_decl']),Regla('decl',['obj_decl']),Regla('decl',['optype_decl']),Regla('decl',['op_decl'])],\n",
    "             'type_decl': [Regla('type_decl',['TK_TYPE','TK_ID','TK_EQ','type','type_restriction'])],\n",
    "             'type_restriction': [Regla('type_restriction',['TK_LBRACE','TK_ID','TK_RBRACE']),Regla('type_restriction',['Ñ'])],\n",
    "             'obj_decl': [Regla('obj_decl',['var_or_const','var_def_lp'])],\n",
    "             'var_or_const': [Regla('var_or_const',['TK_VAR']),Regla('var_or_const',['TK_CONST'])],\n",
    "             'var_def_lp': [Regla('var_def_lp',['var_def','var_def_lp2'])],\n",
    "             'var_def_lp2': [Regla('var_def_lp2',['Ñ']),Regla('var_def_lp2',['TK_COMMA','var_def','var_def_lp2'])],\n",
    "             'var_def': [Regla('var_def',['id_subs_lp','var_att'])],\n",
    "             'var_att': [Regla('var_att',['TK_COLON','type','var_att2']),Regla('var_att',['TK_SEPARATOR']),Regla('var_att',['TK_ASSIGN','expr'])],\n",
    "             'var_att2': [Regla('var_att2',['TK_ASSIGN','expr']),Regla('var_att2',['Ñ'])],\n",
    "             'id_subs_lp': [Regla('id_subs_lp',['id_subs','id_subs_lp2'])],\n",
    "             'id_subs_lp2': [Regla('id_subs_lp2',['TK_COMMA','id_subs','id_subs_lp2']),Regla('id_subs_lp2',['Ñ'])],\n",
    "             'id_subs': [Regla('id_subs',['TK_ID','id_subs2'])],\n",
    "             'id_subs2': [Regla('id_subs2',['subscripts']),Regla('id_subs2',['Ñ'])],\n",
    "             'subscripts': [Regla('subscripts',['bracketed_list','subscripts2'])],\n",
    "             'subscripts2': [Regla('subscripts2',['subscripts']),Regla('subscripts2',['Ñ'])],\n",
    "             'bracketed_list': [Regla('bracketed_list',['TK_LBRACKET','bound_lp','TK_RBRACKET'])],\n",
    "             'bound_lp': [Regla('bound_lp',['bounds','bound_lp2'])],\n",
    "             'bound_lp2': [Regla('bound_lp2',['TK_COMMA','bounds','bound_lp2']),Regla('bound_lp2',['Ñ'])],\n",
    "             'bounds': [Regla('bounds',['bound','bounds2'])],\n",
    "             'bounds2': [Regla('bounds2',['TK_COLON','bound']),Regla('bounds2',['Ñ'])],\n",
    "             'bound': [Regla('bound',['expr']),Regla('bound',['TK_ASTER'])],\n",
    "\n",
    "             'expr': [Regla('expr',['TK_ID','expr2']),Regla('expr',['num','expr2'])],\n",
    "             'num': [Regla('num',['TK_INT']),Regla('num',['TK_REAL'])],\n",
    "             'expr2': [Regla('expr2',['operator','expr']),Regla('expr2',['Ñ'])],\n",
    "             'operator': [Regla('operator',['TK_PLUS']),Regla('operator',['TK_MINUS'])],\n",
    "             #'binary_expr': [Regla('binary_expr',['operator','expr'])],\n",
    "             #'binary_expr2': [Regla('binary_expr2',['TK_PLUS','expr']),Regla('binary_expr2',['TK_MINUS','expr'])],\n",
    "             \n",
    "             'optype_decl': [Regla('optype_decl',['TK_ID'])],\n",
    "             'op_decl': [Regla('op_decl',['TK_ID'])],\n",
    "             'error': [Regla('error',['TK_ID'])],\n",
    "             'type': [Regla('type',['TK_ID'])],\n",
    "             'comp_params': [Regla('comp_params',['TK_ID'])],             \n",
    "             'parameters': [Regla('parameters',['TK_ID'])],\n",
    "             'stmt': [Regla('stmt',['TK_ID'])],\n",
    "             'proc': [Regla('proc',['TK_ID'])],\n",
    "             'process': [Regla('process',['TK_ID'])],\n",
    "             'procedure': [Regla('procedure',['TK_ID'])],\n",
    "             'initial_block': [Regla('initial_block',['TK_ID'])],\n",
    "             'final_block': [Regla('final_block',['TK_ID'])]\n",
    "             \n",
    "             \n",
    "}\n",
    "gramatica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipo_tk_operador = dict([('|', 'TK_OR'), ('&', 'TK_AND'), ('?', 'TK_QMARK'), ('@', 'TK_ADDR'), ('=', 'TK_EQ'), ('<', 'TK_LT'), ('~', 'TK_NOT'), ('>', 'TK_GT'), ('-', 'TK_MINUS'), ('+', 'TK_PLUS'), ('*', 'TK_ASTER'), ('/', 'TK_DIV'), ('^', 'TK_HAT'),\n",
    "                  ('%', 'TK_REMDR'), ('.', 'TK_PERIOD'), (',', 'TK_COMMA'), (';', 'TK_SEPARATOR'), (':', 'TK_COLON'), ('{', 'TK_LBRACE'), ('}', 'TK_RBRACE'), ('(', 'TK_LPAREN')\n",
    "                  , (')', 'TK_RPAREN'), ('[', 'TK_LBRACKET'), (']', 'TK_RBRACKET'),('...', 'TK_THREE_PERIODS'), (':=:', 'TK_SWAP'), ('--', 'TK_DECR'), ('++', 'TK_INCR'), ('**', 'TK_EXPON'), (':=', 'TK_ASSIGN'), ('->', 'TK_ARROW'), ('[]', 'TK_SQUARE'), \n",
    "                ('//', 'TK_PARALLEL'), ('<=', 'TK_LE'),('>=', 'TK_GE'),('!=', 'TK_NE'),('Fin de archivo', 'ÑÑ')])\n",
    "tipo_tk_operador_inv = dict((v,k) for k,v in tipo_tk_operador.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexer():\n",
    "    fila = 1\n",
    "    col = 1\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    with open(r'C:\\Users\\corre\\Desktop\\my_file.txt','r') as file:\n",
    "        codigoFuente = file.read()\n",
    "\n",
    "    #CaseSensitive\n",
    "    pSaltoDeLinea = re.compile(r'\\n', re.M)\n",
    "    pComentarios = re.compile(r'#(.*)?')\n",
    "    pCadena = re.compile(r'(\"[^\"]*\")|(\\'[^\\']*\\')|(“[^”]*”)')\n",
    "    pReservadas = re.compile(r'\\btrue\\b | \\bfalse\\b | \\baf\\b | \\breal\\b | \\bglobal\\b | \\bnext\\b | \\bstring\\b | \\bvar\\b | \\bint\\b | \\bbody\\b | \\bgetarg\\b | \\bif\\b | \\bmod\\b | \\bwrite\\b | \\bwrites\\b | \\bread\\b | \\bstop\\b | \\bfi\\b | \\bend\\b | \\bresource\\b | \\bprocedure\\b | \\breturns\\b | \\bor\\b | \\band\\b | \\bfa\\b | \\bto\\b | \\bimport\\b | \\bcap\\b | \\bcreate\\b | \\bsend\\b | \\binitial\\b | \\bfinal\\b | \\bop\\b | \\bnew\\b | \\bexternal\\b  | \\bproc\\b | \\belse\\b | \\bextend\\b | \\bdestroy\\b | \\bon\\b | \\bnull\\b | \\bnoop\\b | \\bin\\b | \\bres\\b | \\bref\\b | \\bby\\b | \\bselect\\b | \\bdo\\b | \\bod\\b | \\bni\\b | \\bprocess\\b | \\breceive\\b | \\bcall\\b | \\bni\\b | \\breply\\b | \\bexit\\b | \\bco\\b | \\boc\\b | \\bwhen\\b | \\bbinding\\b | \\boptype\\b | \\bst\\b | \\bfile\\b | \\bbool\\b | \\bfrom\\b | \\bchar\\b | \\bconst\\b | \\btype\\b | \\brec\\b | \\bprivate\\b | \\babort\\b | \\bselect\\b | \\bthen\\b | \\bcoenter\\b | \\bcoenter\\b', re.X)\n",
    "    pIdentificador = re.compile(r'\\b[_a-zA-Z][\\w_]{0,31}\\b')\n",
    "    pEntero = re.compile(r'[-+]?\\d+((E-|E\\+|e-|e\\+)\\d+)?')\n",
    "    pReal = re.compile(r'\\d+\\.\\d+((E-|E\\+|e-|e\\+)\\d+)?')\n",
    "    pOperadores3 = re.compile(r'(\\:\\=\\:)|(\\.\\.\\.)')\n",
    "    pOperadores2 = re.compile(r'(\\*\\*)|(\\-\\-)|(\\+\\+)|(\\/\\/)|(\\<\\=)|(\\>\\=)|(\\<\\>)|(\\:\\=)|(\\-\\>)|(\\[\\])|(\\!\\=)')\n",
    "    pOperadores = re.compile(r'[\\|\\&\\?\\@\\~\\+\\-\\*\\/\\^\\=\\<\\>\\%\\.\\,\\;\\:\\{\\}\\[\\]\\(\\)]')\n",
    "    pEspacio = re.compile(r'\\s')   \n",
    "\n",
    "\n",
    "    while (codigoFuente != ''):\n",
    "\n",
    "      if(pSaltoDeLinea.match(codigoFuente)):\n",
    "          lex = pSaltoDeLinea.match(codigoFuente).group()\n",
    "          codigoFuente = codigoFuente[len(lex):]\n",
    "          fila += 1\n",
    "          col = 1\n",
    "\n",
    "      elif (pComentarios.match(codigoFuente)):\n",
    "        lex = pComentarios.match(codigoFuente).group()\n",
    "        codigoFuente = codigoFuente[len(lex):]\n",
    "\n",
    "      elif (pCadena.match(codigoFuente)):\n",
    "        lex = pCadena.match(codigoFuente).group()\n",
    "        codigoFuente = codigoFuente[len(lex):]\n",
    "        token =  Token(lex, 'TK_STRING', fila, col)\n",
    "        tokens.append(token)\n",
    "        col = col + len(lex)\n",
    "        print(\"<\" + token.tipo + \",\" + token.lexema + \",\" + str(token.fila) + \",\" + str(token.col) + \">\")\n",
    "\n",
    "      elif (pReservadas.match(codigoFuente)):\n",
    "        lex = pReservadas.match(codigoFuente).group()\n",
    "        codigoFuente = codigoFuente[len(lex):]\n",
    "        token =  Token(lex, ('TK_'+lex.upper()), fila, col)\n",
    "        tokens.append(token)\n",
    "        col = col + len(lex)\n",
    "        print(\"<\" + token.tipo + \",\" + str(token.fila) + \",\" + str(token.col) + \">\")\n",
    "\n",
    "      elif (pIdentificador.match(codigoFuente)):\n",
    "          lex = pIdentificador.match(codigoFuente).group()\n",
    "          codigoFuente = codigoFuente[len(lex):]\n",
    "          token = Token(lex, 'TK_ID', fila, col)\n",
    "          tokens.append(token)\n",
    "          col = col + len(lex)\n",
    "          print(\"<\" + token.tipo + \",\" + token.lexema + \",\" + str(token.fila) + \",\" + str(token.col) + \">\")\n",
    "          if (pOperadores.match(codigoFuente)):\n",
    "            lex = pOperadores.match(codigoFuente).group()\n",
    "            codigoFuente = codigoFuente[len(lex):]\n",
    "            token = Token(lex, tipo_tk_operador[lex], fila, col)\n",
    "            tokens.append(token)\n",
    "            col = col + len(lex)\n",
    "            print(\"<\" + token.tipo + \",\" + str(token.fila) + \",\" + str(token.col) + \">\")\n",
    "\n",
    "      elif (pReal.match(codigoFuente)):\n",
    "        lex = pReal.match(codigoFuente).group()\n",
    "        codigoFuente = codigoFuente[len(lex):]\n",
    "        token = Token(lex, 'TK_REAL', fila, col)\n",
    "        tokens.append(token)\n",
    "        col = col + len(lex)\n",
    "        print(\"<\" + token.tipo + \",\" + token.lexema + \",\" + str(token.fila) + \",\" + str(token.col) + \">\")\n",
    "        if (pOperadores.match(codigoFuente)):\n",
    "          lex = pOperadores.match(codigoFuente).group()\n",
    "          codigoFuente = codigoFuente[len(lex):]\n",
    "          token = Token(lex, tipo_tk_operador[lex], fila, col)\n",
    "          tokens.append(token)\n",
    "          col = col + len(lex)\n",
    "          print(\"<\" + token.tipo + \",\" + str(token.fila) + \",\" + str(token.col) + \">\")\n",
    "\n",
    "      elif (pEntero.match(codigoFuente)):\n",
    "        lex = pEntero.match(codigoFuente).group()\n",
    "        codigoFuente = codigoFuente[len(lex):]\n",
    "        token = Token(lex, 'TK_INT', fila, col)\n",
    "        tokens.append(token)\n",
    "        col = col + len(lex)\n",
    "        print(\"<\" + token.tipo + \",\" + token.lexema + \",\" + str(token.fila) + \",\" + str(token.col) + \">\")\n",
    "        if (pOperadores.match(codigoFuente)):\n",
    "          lex = pOperadores.match(codigoFuente).group()\n",
    "          codigoFuente = codigoFuente[len(lex):]\n",
    "          token = Token(lex, tipo_tk_operador[lex], fila, col)\n",
    "          tokens.append(token)\n",
    "          col = col + len(lex)\n",
    "          print(\"<\" + token.tipo + \",\" + str(token.fila) + \",\" + str(token.col) + \">\")\n",
    "\n",
    "      elif (pOperadores3.match(codigoFuente)):\n",
    "        lex = pOperadores3.match(codigoFuente).group()\n",
    "        codigoFuente = codigoFuente[len(lex):]\n",
    "        token = Token(lex, tipo_tk_operador[lex], fila, col)\n",
    "        tokens.append(token)\n",
    "        col = col + len(lex)\n",
    "        print(\"<\" + token.tipo + \",\" + str(token.fila) + \",\" + str(token.col) + \">\")\n",
    "\n",
    "      elif (pOperadores2.match(codigoFuente)):\n",
    "        lex = pOperadores2.match(codigoFuente).group()\n",
    "        codigoFuente = codigoFuente[len(lex):]\n",
    "        token = Token(lex, tipo_tk_operador[lex], fila, col)\n",
    "        tokens.append(token)\n",
    "        col = col + len(lex)\n",
    "        print(\"<\" + token.tipo + \",\" + str(token.fila) + \",\" + str(token.col) + \">\")\n",
    "\n",
    "      elif (pOperadores.match(codigoFuente)):\n",
    "        lex = pOperadores.match(codigoFuente).group()\n",
    "        codigoFuente = codigoFuente[len(lex):]\n",
    "        token =  Token(lex, tipo_tk_operador[lex], fila, col)\n",
    "        tokens.append(token)\n",
    "        col = col + len(lex)\n",
    "        print(\"<\" + token.tipo + \",\" + str(token.fila) + \",\" + str(token.col) + \">\")\n",
    "\n",
    "      elif (pEspacio.match(codigoFuente)):\n",
    "        lex = pEspacio.match(codigoFuente).group()\n",
    "        codigoFuente = codigoFuente[len(lex):]\n",
    "        col = col + 1\n",
    "\n",
    "      else:\n",
    "          print(\">>> Error lexico(linea:\" + str(fila) + \",\" + \"posicion:\" + str(col) + \")\")\n",
    "          break\n",
    "\n",
    "    token = Token('ÑÑ', 'ÑÑ', -1, -1)\n",
    "    tokens.append(token)    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_regla = Regla('expr',['binary_expr'])\n",
    "#conj_pred = new_regla.conjunto_prediccion(gramatica)\n",
    "#conj_pred\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regla_a_aplicar(gramatica,no_terminal,token):    \n",
    "    reglas = gramatica[no_terminal]    \n",
    "    for regla in reglas: \n",
    "        conj_pred_regla = regla.conjunto_prediccion(gramatica)\n",
    "        if token.tipo in conj_pred_regla:\n",
    "            return regla\n",
    "    return None\n",
    "\n",
    "def sintaxError1(no_terminal,gramatica):\n",
    "    simbolosEsperados = []\n",
    "    conj_pred_regla = set()\n",
    "    reglas = gramatica[no_terminal]    \n",
    "    for regla in reglas: \n",
    "        conj_pred_regla = conj_pred_regla | regla.conjunto_prediccion(gramatica)\n",
    "    for no_trmnl in conj_pred_regla:\n",
    "        if no_trmnl in tipo_tk_operador_inv:\n",
    "            simbolosEsperados.append(tipo_tk_operador_inv[no_trmnl])\n",
    "        else:\n",
    "            simbolosEsperados.append(no_trmnl)        \n",
    "    return simbolosEsperados\n",
    "        \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivar2(regla):\n",
    "    global tokens\n",
    "    global token_index\n",
    "    print(\"regla1:\")\n",
    "    print(regla.derivacion)\n",
    "    i = 0\n",
    "    while i < len(regla.derivacion):  \n",
    "        token = tokens[token_index]\n",
    "        #print(\"i1:\")\n",
    "        #print(i)\n",
    "        #print(\"regla2:\")\n",
    "        #print(regla.derivacion)\n",
    "        simbolo = regla.derivacion[i]\n",
    "        #print(\"simbolo\")\n",
    "        #print(simbolo)\n",
    "        if simbolo.isupper():            \n",
    "            #print(\"token1\")\n",
    "            #print(token.tipo)\n",
    "            #print(\"tokens_index\")\n",
    "            #print(token_index)\n",
    "            if simbolo == 'Ñ':\n",
    "                i = i + 1\n",
    "            else:\n",
    "                if simbolo == token.tipo:\n",
    "                    i = i + 1\n",
    "                    token_index = token_index + 1                \n",
    "                    #print(\"token2\")\n",
    "                    #print(token.tipo)\n",
    "                    #print(\"i\")\n",
    "                    #print(i)\n",
    "                else:\n",
    "                    if simbolo == 'TK_RESOURCE':\n",
    "                        print('Error sintactico: falta funcion_principal')\n",
    "                    else:\n",
    "                        print(\"<\" + str(token.fila) +\",\" + str(token.col) + \"> Error sintactico: se encontro: \" + token.lexema +\" ; se esperaba: \" + simbolo)\n",
    "                    return True                \n",
    "        else:\n",
    "            no_terminal = simbolo\n",
    "            #print(\"tokens_index4:\")\n",
    "            #print(token_index)\n",
    "            #print(\"noterminal\")\n",
    "            #print(no_terminal)\n",
    "            #print(\"token4\")\n",
    "            #print(token.tipo)\n",
    "            siguiente_regla = regla_a_aplicar(gramatica,no_terminal,token)\n",
    "            if siguiente_regla == None:                \n",
    "                simbolos_esperados = sintaxError1(no_terminal,gramatica)                    \n",
    "                #print(\"error sintactico2\")\n",
    "                print(\"<\" + str(token.fila) +\",\" + str(token.col) + \"> Error sintactico: se encontro: \" + token.lexema +\" ; se esperaba: \" + str(simbolos_esperados))\n",
    "                return True                \n",
    "            else:\n",
    "                #print(\"regla a derivar:\")\n",
    "                #print(regla.derivacion)\n",
    "                error = derivar2(siguiente_regla)\n",
    "                i = i + 1\n",
    "                #print(\"i3\")\n",
    "                #print(i)\n",
    "                #print(\"regla4\")\n",
    "                #print(siguiente_regla.derivacion)\n",
    "                if error:\n",
    "                    return True                \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TK_GLOBAL,3,1>\n",
      "<TK_ID,sizes,3,8>\n",
      "<TK_VAR,4,1>\n",
      "<TK_ID,N,4,5>\n",
      "<TK_ASSIGN,4,7>\n",
      "<TK_INT,10,4,10>\n",
      "<TK_SEPARATOR,4,12>\n",
      "<TK_VAR,5,1>\n",
      "<TK_ID,PR,5,5>\n",
      "<TK_ASSIGN,5,8>\n",
      "<TK_INT,2,5,11>\n",
      "<TK_SEPARATOR,5,12>\n",
      "<TK_END,6,1>\n",
      "<TK_RESOURCE,8,1>\n",
      "<TK_ID,a,8,10>\n",
      "<TK_CONST,9,5>\n",
      "<TK_ID,N,9,11>\n",
      "<TK_COLON,9,13>\n",
      "<TK_INT,12,9,15>\n",
      "<TK_SEPARATOR,9,17>\n",
      "<TK_END,10,1>\n",
      "<TK_RESOURCE,12,1>\n",
      "<TK_ID,b,12,10>\n",
      "<TK_EXTEND,13,5>\n",
      "<TK_ID,a,13,12>\n",
      "<TK_SEPARATOR,13,13>\n",
      "<TK_CONST,14,5>\n",
      "<TK_ID,M,14,11>\n",
      "<TK_ASSIGN,14,13>\n",
      "<TK_INT,33,14,16>\n",
      "<TK_PLUS,14,18>\n",
      "<TK_INT,7,14,19>\n",
      "<TK_PLUS,14,20>\n",
      "<TK_ID,N,14,21>\n",
      "<TK_PLUS,14,22>\n",
      "<TK_INT,2,14,23>\n",
      "<TK_SEPARATOR,14,24>\n",
      "<TK_CONST,15,5>\n",
      "<TK_ID,L,15,11>\n",
      "<TK_ASSIGN,15,13>\n",
      "<TK_ID,M,15,16>\n",
      "<TK_PLUS,15,17>\n",
      "<TK_INT,3,15,18>\n",
      "<TK_SEPARATOR,15,19>\n",
      "<TK_IMPORT,16,5>\n",
      "<TK_ID,a,16,12>\n",
      "<TK_SEPARATOR,16,13>\n",
      "<TK_CONST,17,5>\n",
      "<TK_ID,K,17,11>\n",
      "<TK_ASSIGN,17,13>\n",
      "<TK_INT,0,17,16>\n",
      "<TK_SEPARATOR,17,17>\n",
      "<TK_END,18,1>\n",
      "<TK_RESOURCE,20,1>\n",
      "<TK_ID,c,20,10>\n",
      "<TK_EXTEND,21,5>\n",
      "<TK_ID,b,21,12>\n",
      "<TK_SEPARATOR,21,13>\n",
      "<TK_BODY,22,1>\n",
      "<TK_LPAREN,22,6>\n",
      "<TK_RPAREN,22,7>\n",
      "<TK_TYPE,23,4>\n",
      "<TK_ID,jkjklj,23,9>\n",
      "<TK_EQ,23,16>\n",
      "<TK_ID,jklj,23,18>\n",
      "<TK_SEPARATOR,23,22>\n",
      "<TK_ID,writejlkj,24,5>\n",
      "<TK_SEPARATOR,24,14>\n",
      "<TK_END,25,1>\n",
      "regla\n",
      "['component', 'component_ls']\n",
      "regla1:\n",
      "['component', 'component_ls']\n",
      "regla1:\n",
      "['spec_component']\n",
      "regla1:\n",
      "['comp_label', 'spec_stmt_ls', 'spec_body']\n",
      "regla1:\n",
      "['comp_kwd', 'TK_ID']\n",
      "regla1:\n",
      "['TK_GLOBAL']\n",
      "regla1:\n",
      "['spec_stmt', 'spec_stmt_ls2']\n",
      "regla1:\n",
      "['common_stmt']\n",
      "regla1:\n",
      "['decl']\n",
      "regla1:\n",
      "['obj_decl']\n",
      "regla1:\n",
      "['var_or_const', 'var_def_lp']\n",
      "regla1:\n",
      "['TK_VAR']\n",
      "regla1:\n",
      "['var_def', 'var_def_lp2']\n",
      "regla1:\n",
      "['id_subs_lp', 'var_att']\n",
      "regla1:\n",
      "['id_subs', 'id_subs_lp2']\n",
      "regla1:\n",
      "['TK_ID', 'id_subs2']\n",
      "regla1:\n",
      "['Ñ']\n",
      "regla1:\n",
      "['Ñ']\n",
      "regla1:\n",
      "['TK_ASSIGN', 'expr']\n",
      "regla1:\n",
      "['num', 'expr2']\n",
      "regla1:\n",
      "['TK_INT']\n",
      "regla1:\n",
      "['Ñ']\n",
      "regla1:\n",
      "['Ñ']\n",
      "regla1:\n",
      "['TK_SEPARATOR', 'spec_stmt', 'spec_stmt_ls2']\n",
      "regla1:\n",
      "['common_stmt']\n",
      "regla1:\n",
      "['decl']\n",
      "regla1:\n",
      "['obj_decl']\n",
      "regla1:\n",
      "['var_or_const', 'var_def_lp']\n",
      "regla1:\n",
      "['TK_VAR']\n",
      "regla1:\n",
      "['var_def', 'var_def_lp2']\n",
      "regla1:\n",
      "['id_subs_lp', 'var_att']\n",
      "regla1:\n",
      "['id_subs', 'id_subs_lp2']\n",
      "regla1:\n",
      "['TK_ID', 'id_subs2']\n",
      "regla1:\n",
      "['Ñ']\n",
      "regla1:\n",
      "['Ñ']\n",
      "regla1:\n",
      "['TK_ASSIGN', 'expr']\n",
      "regla1:\n",
      "['num', 'expr2']\n",
      "regla1:\n",
      "['TK_INT']\n",
      "regla1:\n",
      "['Ñ']\n",
      "regla1:\n",
      "['Ñ']\n",
      "regla1:\n",
      "['TK_SEPARATOR', 'spec_stmt', 'spec_stmt_ls2']\n",
      "regla1:\n",
      "['common_stmt']\n",
      "regla1:\n",
      "['Ñ']\n",
      "regla1:\n",
      "['Ñ']\n",
      "regla1:\n",
      "['end_id']\n",
      "regla1:\n",
      "['TK_END', 'id_opt']\n",
      "regla1:\n",
      "['Ñ']\n",
      "regla1:\n",
      "['component', 'component_ls']\n",
      "regla1:\n",
      "['spec_component']\n",
      "regla1:\n",
      "['comp_label', 'spec_stmt_ls', 'spec_body']\n",
      "regla1:\n",
      "['comp_kwd', 'TK_ID']\n",
      "regla1:\n",
      "['TK_RESOURCE']\n",
      "regla1:\n",
      "['spec_stmt', 'spec_stmt_ls2']\n",
      "regla1:\n",
      "['common_stmt']\n",
      "regla1:\n",
      "['decl']\n",
      "regla1:\n",
      "['obj_decl']\n",
      "regla1:\n",
      "['var_or_const', 'var_def_lp']\n",
      "regla1:\n",
      "['TK_CONST']\n",
      "regla1:\n",
      "['var_def', 'var_def_lp2']\n",
      "regla1:\n",
      "['id_subs_lp', 'var_att']\n",
      "regla1:\n",
      "['id_subs', 'id_subs_lp2']\n",
      "regla1:\n",
      "['TK_ID', 'id_subs2']\n",
      "regla1:\n",
      "['Ñ']\n",
      "regla1:\n",
      "['Ñ']\n",
      "regla1:\n",
      "['TK_COLON', 'type', 'var_att2']\n",
      "<9,15> Error sintactico: se encontro: 12 ; se esperaba: ['TK_ID']\n"
     ]
    }
   ],
   "source": [
    "tokens = lexer()\n",
    "token_index = 0\n",
    "simbolo_inicial = list(gramatica.keys())[0]\n",
    "token = tokens[token_index]\n",
    "siguiente_regla = regla_a_aplicar(gramatica,simbolo_inicial,token)\n",
    "print(\"regla\")\n",
    "print(siguiente_regla.derivacion)\n",
    "if siguiente_regla == None:\n",
    "    #print(\"error sintactico2\")\n",
    "    simbolos_esperados = sintaxError1(no_terminal,gramatica)\n",
    "    print(\"<\" + str(token.fila) +\",\" + str(token.col) + \"> Error sintactico: se encontro: \" + token.lexema +\" ; se esperaba: \" + str(simbolos_esperados))    \n",
    "else:\n",
    "    error = derivar2(siguiente_regla)\n",
    "    if not error:\n",
    "        if tokens[token_index].tipo != 'ÑÑ':    \n",
    "            print(\"error sintactico, se esperaba fin de archivo\")\n",
    "        else:\n",
    "            print(\"El analisis sintactico ha finalizado exitosamente.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
